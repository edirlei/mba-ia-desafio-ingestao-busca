import os
from dotenv import load_dotenv

from langchain_postgres import PGVector
from langchain_google_genai import (
    GoogleGenerativeAI,
    GoogleGenerativeAIEmbeddings
)
from langchain_core.prompts import PromptTemplate

load_dotenv()

DATABASE_URL = os.getenv("DATABASE_URL")
COLLECTION_NAME = os.getenv("PG_VECTOR_COLLECTION_NAME", "pdf_documents")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
EMBEDDING_MODEL = os.getenv("GOOGLE_EMBEDDING_MODEL", "models/embedding-001")
LLM_MODEL = os.getenv("GOOGLE_LLM_MODEL", "gemini-2.5-flash-lite")

PROMPT_TEMPLATE = """
CONTEXTO:
{contexto}

REGRAS:
- Responda somente com base no CONTEXTO.
- Se a informa√ß√£o n√£o estiver explicitamente no CONTEXTO, responda:
  "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."
- Nunca invente ou use conhecimento externo.
- Nunca produza opini√µes ou interpreta√ß√µes al√©m do que est√° escrito.

PERGUNTA DO USU√ÅRIO:
{pergunta}

RESPONDA A "PERGUNTA DO USU√ÅRIO"
"""

def search_prompt(pergunta: str):
    if not pergunta:
        return None

    # üîπ Embeddings (OBRIGAT√ìRIO tamb√©m na busca)
    embeddings = GoogleGenerativeAIEmbeddings(
        model=EMBEDDING_MODEL,
        google_api_key=GOOGLE_API_KEY
    )

    # üîπ Conex√£o com o banco vetorial
    vectorstore = PGVector(
        connection=DATABASE_URL,
        collection_name=COLLECTION_NAME,
        embeddings=embeddings
    )

    # üîπ Busca sem√¢ntica (k=10 ‚Äì exig√™ncia do edital)
    results = vectorstore.similarity_search_with_score(pergunta, k=10)

    if not results:
        contexto = ""
    else:
        contexto = "\n\n".join([doc.page_content for doc, _ in results])

    prompt = PromptTemplate(
        template=PROMPT_TEMPLATE,
        input_variables=["contexto", "pergunta"]
    ).format(contexto=contexto, pergunta=pergunta)

    llm = GoogleGenerativeAI(
        model=LLM_MODEL,
        google_api_key=GOOGLE_API_KEY,
        temperature=0
    )

    return llm.invoke(prompt)
